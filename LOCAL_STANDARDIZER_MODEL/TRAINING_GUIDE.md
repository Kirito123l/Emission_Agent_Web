# è®­ç»ƒæ“ä½œæŒ‡å—

## ğŸ¯ ä½ ç°åœ¨çš„ä½ç½®

âœ… **æ•°æ®å‡†å¤‡å®Œæˆ** - 8,121æ¡é«˜è´¨é‡è®­ç»ƒæ•°æ®å·²å°±ç»ª
â³ **æ¥ä¸‹æ¥**: ç¯å¢ƒè®¾ç½® â†’ æ¨¡å‹è®­ç»ƒ â†’ è¯„ä¼° â†’ é›†æˆ

## ğŸ“ å®Œæ•´æ“ä½œæ­¥éª¤

### æ­¥éª¤1: ç¯å¢ƒå‡†å¤‡ï¼ˆå¿…éœ€ï¼‰

#### æ–¹æ¡ˆA: ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆæ¨èï¼‰

**Windowsç”¨æˆ·**:
```bash
# åœ¨ LOCAL_STANDARDIZER_MODEL ç›®å½•ä¸‹è¿è¡Œ
setup_environment.bat
```

**Linux/Macç”¨æˆ·**:
```bash
# åœ¨ LOCAL_STANDARDIZER_MODEL ç›®å½•ä¸‹è¿è¡Œ
bash setup_environment.sh
```

#### æ–¹æ¡ˆB: æ‰‹åŠ¨è®¾ç½®

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n local_standardizer python=3.10 -y

# 2. æ¿€æ´»ç¯å¢ƒ
conda activate local_standardizer

# 3. å®‰è£…PyTorchï¼ˆæ ¹æ®ä½ çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# æˆ– CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# æˆ– CPU onlyï¼ˆä¸æ¨èï¼Œè®­ç»ƒä¼šå¾ˆæ…¢ï¼‰
pip install torch torchvision torchaudio

# 4. å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers==4.36.0 peft==0.7.1 datasets==2.14.0 accelerate==0.25.0 pyyaml tqdm bitsandbytes

# 5. éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

**æœŸæœ›è¾“å‡º**:
```
PyTorch: 2.x.x
CUDA: True  # å¦‚æœæœ‰GPU
```

### æ­¥éª¤2: è®­ç»ƒç»Ÿä¸€æ¨¡å‹ï¼ˆè½¦å‹+æ±¡æŸ“ç‰©ï¼‰

#### æ–¹æ¡ˆA: ä½¿ç”¨å¿«æ·è„šæœ¬ï¼ˆæ¨èï¼‰

**Windows**:
```bash
train_unified.bat
```

**Linux/Mac**:
```bash
conda activate local_standardizer
python scripts/04_train_lora.py --config configs/unified_lora_config.yaml --model_type unified
```

#### è®­ç»ƒå‚æ•°è¯´æ˜

```yaml
åŸºç¡€æ¨¡å‹: Qwen/Qwen2.5-3B-Instruct (çº¦6GB)
LoRA rank: 16
è®­ç»ƒè½®æ•°: 5 epochs
å­¦ä¹ ç‡: 2e-4
æ‰¹æ¬¡å¤§å°: 4 Ã— 4 (gradient accumulation)
æ•°æ®é‡: 4,352æ¡è®­ç»ƒ + 512æ¡éªŒè¯
```

#### é¢„è®¡æ—¶é—´

| GPUå‹å· | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜å ç”¨ |
|---------|----------|----------|
| RTX 3090 (24GB) | 2-3å°æ—¶ | ~12GB |
| RTX 4090 (24GB) | 1-2å°æ—¶ | ~12GB |
| RTX 3080 (10GB) | 3-4å°æ—¶ | ~9GB |
| CPU only | 20-30å°æ—¶ | N/A |

#### è®­ç»ƒè¿‡ç¨‹ç›‘æ§

è®­ç»ƒæ—¶ä¼šçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š
```
[1/6] åŠ è½½æ¨¡å‹å’Œ tokenizer...
  - åŸºç¡€æ¨¡å‹: Qwen/Qwen2.5-3B-Instruct
  - æ¨¡å‹å‚æ•°é‡: 3.09B

[2/6] é…ç½® LoRA...
trainable params: 8,388,608 || all params: 3,098,388,608 || trainable%: 0.27%

[3/6] åŠ è½½æ•°æ®é›†...
  - è®­ç»ƒé›†: 4352 æ¡
  - éªŒè¯é›†: 512 æ¡

[4/6] é¢„å¤„ç†æ•°æ®...
é¢„å¤„ç†è®­ç»ƒé›†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4352/4352

[5/6] é…ç½®è®­ç»ƒå‚æ•°...

[6/6] å¼€å§‹è®­ç»ƒ...
Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 272/272 [12:34<00:00, 2.77s/it, loss=0.234]
Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 272/272 [12:31<00:00, 2.76s/it, loss=0.156]
...
```

#### è¾“å‡ºä½ç½®

```
models/unified_lora/
â”œâ”€â”€ checkpoint-100/
â”œâ”€â”€ checkpoint-200/
â”œâ”€â”€ checkpoint-300/
â””â”€â”€ final/              # æœ€ç»ˆæ¨¡å‹ï¼ˆç”¨äºè¯„ä¼°å’Œéƒ¨ç½²ï¼‰
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.bin
    â””â”€â”€ ...
```

### æ­¥éª¤3: è®­ç»ƒåˆ—åæ˜ å°„æ¨¡å‹

#### æ–¹æ¡ˆA: ä½¿ç”¨å¿«æ·è„šæœ¬

**Windows**:
```bash
train_column.bat
```

**Linux/Mac**:
```bash
conda activate local_standardizer
python scripts/04_train_lora.py --config configs/column_lora_config.yaml --model_type column
```

#### è®­ç»ƒå‚æ•°è¯´æ˜

```yaml
åŸºç¡€æ¨¡å‹: Qwen/Qwen2.5-3B-Instruct
LoRA rank: 32 (æ›´å¤§ï¼Œå› ä¸ºä»»åŠ¡æ›´å¤æ‚)
è®­ç»ƒè½®æ•°: 8 epochs
å­¦ä¹ ç‡: 1e-4
æ•°æ®é‡: 2,550æ¡è®­ç»ƒ + 300æ¡éªŒè¯
```

#### é¢„è®¡æ—¶é—´

| GPUå‹å· | è®­ç»ƒæ—¶é—´ |
|---------|----------|
| RTX 3090 | 1-2å°æ—¶ |
| RTX 4090 | 30-60åˆ†é’Ÿ |
| RTX 3080 | 2-3å°æ—¶ |

### æ­¥éª¤4: è¯„ä¼°æ¨¡å‹

```bash
# è¯„ä¼°ç»Ÿä¸€æ¨¡å‹
python scripts/06_evaluate.py \
    --model_type unified \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_path models/unified_lora/final

# è¯„ä¼°åˆ—åæ˜ å°„æ¨¡å‹
python scripts/06_evaluate.py \
    --model_type column \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --lora_path models/column_lora/final
```

#### ç›®æ ‡å‡†ç¡®ç‡

- âœ… è½¦å‹æ ‡å‡†åŒ–: â‰¥95%
- âœ… æ±¡æŸ“ç‰©æ ‡å‡†åŒ–: â‰¥98%
- âœ… åˆ—åæ˜ å°„: â‰¥90%

### æ­¥éª¤5: é›†æˆåˆ°emission_agent

å‚è€ƒ `INTEGRATION_ANALYSIS.md` æ–‡æ¡£ï¼š

1. åˆ›å»º `shared/standardizer/local_client.py`
2. ä¿®æ”¹é…ç½®æ–‡ä»¶
3. æ›´æ–°æ ‡å‡†åŒ–å™¨åˆå§‹åŒ–
4. æµ‹è¯•é›†æˆ

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œå‡å°æ‰¹æ¬¡å¤§å°
per_device_train_batch_size: 2  # ä»4æ”¹ä¸º2
gradient_accumulation_steps: 8  # ä»4æ”¹ä¸º8
```

### Q2: è®­ç»ƒé€Ÿåº¦æ…¢

**æ£€æŸ¥**:
```python
import torch
print(torch.cuda.is_available())  # åº”è¯¥æ˜¯ True
print(torch.cuda.get_device_name(0))  # æŸ¥çœ‹GPUå‹å·
```

### Q3: æ¨¡å‹ä¸‹è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½®é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶åä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ base_model è·¯å¾„
```

### Q4: å‡†ç¡®ç‡ä¸è¾¾æ ‡

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ è®­ç»ƒè½®æ•°
2. è°ƒæ•´å­¦ä¹ ç‡
3. å¢åŠ è®­ç»ƒæ•°æ®
4. æ£€æŸ¥æ•°æ®è´¨é‡

## ğŸ“Š è®­ç»ƒè¿›åº¦è¿½è¸ª

åˆ›å»ºä¸€ä¸ªæ£€æŸ¥æ¸…å•ï¼š

- [ ] ç¯å¢ƒè®¾ç½®å®Œæˆ
- [ ] PyTorch + CUDA éªŒè¯é€šè¿‡
- [ ] ç»Ÿä¸€æ¨¡å‹è®­ç»ƒå®Œæˆ
- [ ] ç»Ÿä¸€æ¨¡å‹è¯„ä¼°é€šè¿‡ï¼ˆå‡†ç¡®ç‡â‰¥95%ï¼‰
- [ ] åˆ—åæ˜ å°„æ¨¡å‹è®­ç»ƒå®Œæˆ
- [ ] åˆ—åæ˜ å°„æ¨¡å‹è¯„ä¼°é€šè¿‡ï¼ˆå‡†ç¡®ç‡â‰¥90%ï¼‰
- [ ] åˆ›å»ºé€‚é…å™¨ç±»
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³æ‰§è¡Œ**:
1. è¿è¡Œ `setup_environment.bat` è®¾ç½®ç¯å¢ƒ
2. éªŒè¯CUDAå¯ç”¨
3. è¿è¡Œ `train_unified.bat` å¼€å§‹è®­ç»ƒ

**é¢„è®¡æ€»æ—¶é—´**: 4-6å°æ—¶ï¼ˆåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒï¼‰

## ğŸ“ è·å–å¸®åŠ©

- æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: `README.md`
- æŸ¥çœ‹é›†æˆåˆ†æ: `INTEGRATION_ANALYSIS.md`
- æŸ¥çœ‹å¿«é€Ÿå¼€å§‹: `QUICKSTART.md`

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
